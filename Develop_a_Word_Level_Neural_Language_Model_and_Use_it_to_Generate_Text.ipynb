{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT+OXl8pV+LXqGySzYBG73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssef893/Develop-a-Word-Level-Neural-Language-Model-and-Use-it-to-Generate-Text/blob/main/Develop_a_Word_Level_Neural_Language_Model_and_Use_it_to_Generate_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This Notebook from https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
      ],
      "metadata": {
        "id": "SXI3z4txrk3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ======================================================================="
      ],
      "metadata": {
        "id": "FYb4rP5PrqDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "import string \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding"
      ],
      "metadata": {
        "id": "6a2aakyzwOgK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Text\n"
      ],
      "metadata": {
        "id": "pWR1xKOnrxWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3HzYarZnn4tl"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load document\n",
        "in_filename = 'republic_clean.txt'\n",
        "doc = load_doc(in_filename)\n",
        "print(doc[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLkvikFFsk4b",
        "outputId": "ace62345-975f-4330-b258-2273ab56ecb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg EBook of The Republic, by Plato\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean text\n",
        "\n",
        "\n",
        "*  Replace ‘–‘ with a white space so we can split words better.\n",
        "*  Split words based on white space.\n",
        "*  Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
        "*  Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
        "*  Normalize all words to lowercase to reduce the vocabulary size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jIXD25NmsuBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ],
      "metadata": {
        "id": "nZ6P-f6AsnOz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqECuNl6tm4j",
        "outputId": "3d12facf-b3ab-4f58-c027-8a26b35bef43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'the', 'republic', 'author', 'plato', 'translator', 'b', 'jowett', 'posting', 'date', 'august', 'ebook', 'release', 'date', 'october', 'last', 'updated', 'june', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'produced', 'by', 'sue', 'asscher', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'the', 'republic', 'by', 'plato', 'jowett', 'etext', 'introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn', 'out', 'in', 'the', 'laws', 'as', 'works', 'of', 'art', 'the', 'symposium', 'and', 'the', 'protagoras', 'are', 'of', 'higher', 'excellence', 'but', 'no', 'other', 'dialogue', 'of', 'plato', 'has', 'the', 'same', 'largeness', 'of', 'view', 'and', 'the', 'same', 'perfection', 'of', 'style', 'no', 'other', 'shows', 'an', 'equal', 'knowledge', 'of']\n",
            "Total Tokens: 216791\n",
            "Unique Tokens: 10454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK1OjCjrtoZz",
        "outputId": "23811c4f-74b0-4dc3-86f9-f942d042d385"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 216740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Sequence"
      ],
      "metadata": {
        "id": "b27uDqSfv5Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "metadata": {
        "id": "9lPdwV_1vmdP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "metadata": {
        "id": "lPkSVWHevobw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Sequence"
      ],
      "metadata": {
        "id": "7N0mRuP6v7ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "metadata": {
        "id": "IHMVN28Zv9Ve"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Sequence"
      ],
      "metadata": {
        "id": "gsgyFRx5wCIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "HoJFqSLdwB1a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "oGfIuybSwZrS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "metadata": {
        "id": "hTGnqmWdzRE5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit Model"
      ],
      "metadata": {
        "id": "FIcwMuoHzUuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu5fmFKJzZ-S",
        "outputId": "f1912132-f63f-4e03-8150-c5da1f6d4e93"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            522750    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10455)             1055955   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,729,605\n",
            "Trainable params: 1,729,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bypv9aYVzrtp",
        "outputId": "6f59ea04-c43a-470a-c76b-0c4a0948141f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1694/1694 [==============================] - 554s 324ms/step - loss: 6.1254 - accuracy: 0.0976\n",
            "Epoch 2/100\n",
            "1694/1694 [==============================] - 567s 335ms/step - loss: 5.6272 - accuracy: 0.1371\n",
            "Epoch 3/100\n",
            "1694/1694 [==============================] - 541s 319ms/step - loss: 5.4145 - accuracy: 0.1541\n",
            "Epoch 4/100\n",
            "1694/1694 [==============================] - 550s 324ms/step - loss: 5.2785 - accuracy: 0.1645\n",
            "Epoch 5/100\n",
            "1694/1694 [==============================] - 537s 317ms/step - loss: 5.1605 - accuracy: 0.1712\n",
            "Epoch 6/100\n",
            "1694/1694 [==============================] - 542s 320ms/step - loss: 5.0567 - accuracy: 0.1763\n",
            "Epoch 7/100\n",
            "1694/1694 [==============================] - 542s 320ms/step - loss: 4.9646 - accuracy: 0.1809\n",
            "Epoch 8/100\n",
            "1694/1694 [==============================] - 539s 318ms/step - loss: 4.8773 - accuracy: 0.1853\n",
            "Epoch 9/100\n",
            "1694/1694 [==============================] - 538s 317ms/step - loss: 4.7990 - accuracy: 0.1898\n",
            "Epoch 10/100\n",
            "1694/1694 [==============================] - 543s 320ms/step - loss: 4.7298 - accuracy: 0.1926\n",
            "Epoch 11/100\n",
            "1694/1694 [==============================] - 540s 319ms/step - loss: 4.6599 - accuracy: 0.1962\n",
            "Epoch 12/100\n",
            "1694/1694 [==============================] - 546s 322ms/step - loss: 4.5985 - accuracy: 0.1996\n",
            "Epoch 13/100\n",
            "1694/1694 [==============================] - 549s 324ms/step - loss: 4.5411 - accuracy: 0.2022\n",
            "Epoch 14/100\n",
            "1694/1694 [==============================] - 553s 326ms/step - loss: 4.4856 - accuracy: 0.2043\n",
            "Epoch 15/100\n",
            "1694/1694 [==============================] - 552s 326ms/step - loss: 4.4332 - accuracy: 0.2071\n",
            "Epoch 16/100\n",
            "1694/1694 [==============================] - 551s 325ms/step - loss: 4.3841 - accuracy: 0.2093\n",
            "Epoch 17/100\n",
            "1694/1694 [==============================] - 550s 325ms/step - loss: 4.3388 - accuracy: 0.2115\n",
            "Epoch 18/100\n",
            "1694/1694 [==============================] - 553s 326ms/step - loss: 4.2965 - accuracy: 0.2142\n",
            "Epoch 19/100\n",
            "1694/1694 [==============================] - 550s 325ms/step - loss: 4.2568 - accuracy: 0.2170\n",
            "Epoch 20/100\n",
            "1694/1694 [==============================] - 552s 326ms/step - loss: 4.2184 - accuracy: 0.2197\n",
            "Epoch 21/100\n",
            "1694/1694 [==============================] - 554s 327ms/step - loss: 4.1832 - accuracy: 0.2229\n",
            "Epoch 22/100\n",
            "1694/1694 [==============================] - 554s 327ms/step - loss: 4.1483 - accuracy: 0.2246\n",
            "Epoch 23/100\n",
            "1694/1694 [==============================] - 553s 327ms/step - loss: 4.1162 - accuracy: 0.2272\n",
            "Epoch 24/100\n",
            "1694/1694 [==============================] - 551s 325ms/step - loss: 4.0856 - accuracy: 0.2303\n",
            "Epoch 25/100\n",
            "1694/1694 [==============================] - 556s 328ms/step - loss: 4.0553 - accuracy: 0.2328\n",
            "Epoch 26/100\n",
            "1694/1694 [==============================] - 560s 331ms/step - loss: 4.0271 - accuracy: 0.2356\n",
            "Epoch 27/100\n",
            "1372/1694 [=======================>......] - ETA: 1:44 - loss: 3.9834 - accuracy: 0.2381"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "VMCCNvzvzu_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "6xR3cOef0EOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "wiy9NTby0C_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}